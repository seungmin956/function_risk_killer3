# ./fda_realtime_crawler.py
import re
import asyncio
import json
import sqlite3
import os
from datetime import datetime,timedelta
from playwright.async_api import async_playwright
from urllib.parse import urljoin
from db_utils import save_to_sqlite, save_to_chromadb

def get_latest_date_from_db():
    """SQLite DBì—ì„œ ê°€ì¥ ìµœì‹  ë‚ ì§œ ì¡°íšŒ"""
    db_path = "./data/fda_recalls.db"
    if not os.path.exists(db_path):
        print("ğŸ“‹ DB íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
        return None
        
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # ğŸ†• ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•´ì„œ ê°€ì¥ ìµœì‹  ë°ì´í„° 1ê°œ ì¡°íšŒ
        cursor.execute("""
            SELECT fda_publish_date 
            FROM recalls 
            WHERE fda_publish_date IS NOT NULL 
            ORDER BY fda_publish_date DESC 
            LIMIT 1
        """)
        
        result = cursor.fetchone()
        conn.close()
        
        if result and result[0]:
            latest_date = result[0]
            print(f"ğŸ“Š DBì—ì„œ ì¡°íšŒëœ ìµœì‹  ë‚ ì§œ: {latest_date}")
            
            # ì´ë¯¸ YYYY-MM-DD í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if len(latest_date) == 10 and latest_date.count('-') == 2:
                return latest_date
            
            # ë‹¤ë¥¸ í˜•ì‹ì´ë©´ ë³€í™˜ ì‹œë„
            try:
                parsed_date = datetime.strptime(latest_date, "%Y-%m-%d")
                return parsed_date.strftime("%Y-%m-%d")
            except:
                print(f"âš ï¸ ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {latest_date}")
                return latest_date  # ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        else:
            print("ğŸ“‹ DBì— ë°ì´í„°ê°€ ì—†ìŒ")
            return None
        
    except Exception as e:
        print(f"DB ìµœì‹  ë‚ ì§œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None
    

async def crawl_incremental_links():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage']
        )
        page = await browser.new_page()

        # ğŸ†• User-Agent ì¶”ê°€ (ë¸Œë¼ìš°ì € ìœ„ì¥)
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        try:
            print("ğŸŒ FDA ì‚¬ì´íŠ¸ ì ‘ì† (í•„í„°ë§ ì—†ì´)...")
            await page.goto("https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/")
            await page.wait_for_load_state('networkidle')
            print("âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            print(f"ğŸ’¥ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
            return []

        base_url = "https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/"
        all_brand_urls = []
        current_page_count = 1

        latest_db_date = get_latest_date_from_db()
        print(f"ğŸ“Š DB ìµœì‹  ë‚ ì§œ: {latest_db_date}")
        
        max_pages = 10  # ì•ˆì „ì¥ì¹˜
        while current_page_count <= max_pages:
            print(f"í˜„ì¬ {current_page_count}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
            
            # ğŸ†• ë‹¤ì–‘í•œ í…Œì´ë¸” ì…€ë ‰í„° ì‹œë„
            table_selectors = [
                "table tbody tr",
                ".view-content .views-row", 
                "table tr",
                ".views-table tbody tr"
            ]
            
            rows_found = False
            for table_selector in table_selectors:
                try:
                    await page.wait_for_selector(table_selector, timeout=15000)
                    rows = await page.locator(table_selector).all()
                    if len(rows) > 0:
                        print(f"âœ… í…Œì´ë¸” ë°œê²¬: {len(rows)}ê°œ í–‰ ({table_selector})")
                        rows_found = True
                        break
                except:
                    continue
            
            if not rows_found:
                print("âš ï¸ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì¢…ë£Œ")
                break
            
            # ğŸ†• ì¡°ê±´ë¶€ ë°ì´í„° ìˆ˜ì§‘
            try:
                # ë‚ ì§œ, ë§í¬, Product Type ë™ì‹œ ìˆ˜ì§‘
                date_elements = await page.locator("td:nth-child(1)").all()  # ë‚ ì§œ
                link_elements = await page.locator("td:nth-child(2) a").all()  # ë§í¬  
                product_type_elements = await page.locator("td:nth-child(4)").all()  # Product Type
                
                print(f"ğŸ“Š ë°œê²¬ëœ ìš”ì†Œ: ë‚ ì§œ {len(date_elements)}ê°œ, ë§í¬ {len(link_elements)}ê°œ, ì œí’ˆíƒ€ì… {len(product_type_elements)}ê°œ")
                
            except Exception as e:
                print(f"âš ï¸ ìš”ì†Œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                break
            
            page_has_new_data = False
            should_break = False
            
            # ğŸ¯ ì¡°ê±´ë¶€ ìˆ˜ì§‘ ë¡œì§
            for i in range(min(len(date_elements), len(link_elements), len(product_type_elements))):
                try:
                    # ë‚ ì§œ ì¶”ì¶œ
                    date_text = await date_elements[i].text_content()
                    date_text = date_text.strip()
                    
                    # ë‚ ì§œ ë³€í™˜
                    date_only = None
                    if 'T' in date_text:
                        date_only = date_text.split('T')[0]
                    elif '/' in date_text:
                        try:
                            parsed_date = datetime.strptime(date_text, "%m/%d/%Y")
                            date_only = parsed_date.strftime("%Y-%m-%d")
                        except:
                            date_only = date_text
                    elif '-' in date_text and len(date_text) == 10:
                        date_only = date_text
                    else:
                        continue  # ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    
                    print(f"  ğŸ“… #{i}: ë‚ ì§œ '{date_only}'")
                    
                    # ğŸš¨ DB ìµœì‹  ë‚ ì§œì™€ ë¹„êµ (ë” ì˜¤ë˜ëœ ë°ì´í„°ë©´ ì¤‘ë‹¨)
                    if latest_db_date and date_only:
                        try:
                            current_date_obj = datetime.strptime(date_only, "%Y-%m-%d")
                            latest_date_obj = datetime.strptime(latest_db_date, "%Y-%m-%d")
                            
                            if current_date_obj <= latest_date_obj:
                                print(f"ğŸ“Š ê¸°ì¡´ DB ë‚ ì§œ ë„ë‹¬: {date_only} (DB ìµœì‹ : {latest_db_date}) - ì¤‘ë‹¨")
                                should_break = True
                                break
                        except Exception as e:
                            print(f"  âš ï¸ ë‚ ì§œ ë¹„êµ ì˜¤ë¥˜: {e}")
                    
                    # Product Type í™•ì¸
                    product_type_text = await product_type_elements[i].text_content()
                    product_type_text = product_type_text.strip().lower()
                    
                    print(f"  ğŸ·ï¸ #{i}: ì œí’ˆíƒ€ì… '{product_type_text[:50]}...'")
                    
                    # ğŸ¯ Food & Beverages ì •í™•í•œ ì¡°ê±´ í™•ì¸
                    # ì½¤ë§ˆë¡œ ë¶„ë¦¬í•´ì„œ ì²« ë²ˆì§¸ê°€ "Food & Beverages"ì¸ì§€ í™•ì¸
                    product_parts = product_type_text.split(',')
                    first_part = product_parts[0].strip()
                    
                    is_food_beverage = first_part.lower() == "food & beverages"
                    
                    print(f"  ğŸ” #{i}: ì²«ë²ˆì§¸ ë¶€ë¶„ '{first_part}' â†’ {'âœ…' if is_food_beverage else 'âŒ'}")
                    
                    if is_food_beverage:
                        # URL ìˆ˜ì§‘
                        url = await link_elements[i].get_attribute("href")
                        brand_name = await link_elements[i].text_content()
                        full_url = urljoin(base_url, url)
                        
                        all_brand_urls.append({
                            "name": brand_name.strip(), 
                            "url": full_url,
                            "date": date_only,
                            "product_type": product_type_text
                        })
                        page_has_new_data = True
                        print(f"  âœ… ìˆ˜ì§‘: {date_only} - {brand_name.strip()}")
                    else:
                        print(f"  â­ï¸ ìŠ¤í‚µ: Food & Beverages ì•„ë‹˜")
                        
                except Exception as e:
                    print(f"  âš ï¸ í•­ëª© {i} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # ğŸ†• í˜ì´ì§€ë³„ ì¡°ê±´ í™•ì¸ í›„ ë‹¤ìŒ í˜ì´ì§€ ê²°ì •
            if should_break:
                print(f"ğŸ”š ê¸°ì¡´ DB ë‚ ì§œ ë„ë‹¬ë¡œ í¬ë¡¤ë§ ì¢…ë£Œ (í˜ì´ì§€ {current_page_count})")
                break
            
            if not page_has_new_data:
                print(f"ğŸ’¡ í˜ì´ì§€ {current_page_count}ì—ì„œ ìƒˆë¡œìš´ Food & Beverages ë°ì´í„° ì—†ìŒ")
                print(f"ğŸ”š ë” ì´ìƒ ì§„í–‰í•  í•„ìš” ì—†ìŒ - í¬ë¡¤ë§ ì¢…ë£Œ")
                break  # ğŸ†• ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ
            
            # ë‹¤ìŒ í˜ì´ì§€ ì´ë™
            try:
                next_selectors = [
                    "a[rel='next']",
                    ".pager-next a",
                    ".pagination .next a",
                    "a:has-text('Next')",
                    "a:has-text('â€º')"
                ]
                
                next_found = False
                for next_selector in next_selectors:
                    try:
                        next_button = page.locator(next_selector)
                        if await next_button.count() > 0:
                            await next_button.click()
                            await page.wait_for_load_state('networkidle')
                            current_page_count += 1
                            next_found = True
                            print(f"  â¡ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ({current_page_count})")
                            break
                    except:
                        continue
                
                if not next_found:
                    print("ğŸ”š ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ - ì¢…ë£Œ")
                    break
                    
            except Exception as e:
                print(f"ğŸ”š í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ - ì¢…ë£Œ: {e}")
                break
        
        # ê²°ê³¼ ì •ë¦¬
        unique_urls = []
        seen_urls = set()
        
        for item in all_brand_urls:
            if item["url"] not in seen_urls:
                seen_urls.add(item["url"])
                unique_urls.append(item)
        
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"ì´ ì²˜ë¦¬ í˜ì´ì§€: {current_page_count}ê°œ")
        print(f"Food & Beverages ë°ì´í„°: {len(all_brand_urls)}ê°œ")
        print(f"ì¤‘ë³µ ì œê±° í›„: {len(unique_urls)}ê°œ")
        
        await browser.close()
        return unique_urls

def check_existing_urls(new_urls):
    """ê¸°ì¡´ DBì—ì„œ URL ì¤‘ë³µ ì²´í¬"""
    db_path = "./data/fda_recalls.db"
    if not os.path.exists(db_path):
        return new_urls  # DB ì—†ìœ¼ë©´ ëª¨ë“  URL ì²˜ë¦¬
        
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    existing_urls = set()
    for url_info in new_urls:
        cursor.execute("SELECT url FROM recalls WHERE url = ?", (url_info["url"],))
        if cursor.fetchone():
            existing_urls.add(url_info["url"])
    
    conn.close()
    
    # ìƒˆë¡œìš´ URLë§Œ í•„í„°ë§
    filtered_urls = [url_info for url_info in new_urls if url_info["url"] not in existing_urls]
    print(f"ğŸ” ì¤‘ë³µ ì²´í¬: {len(new_urls)}ê°œ â†’ {len(filtered_urls)}ê°œ (ìƒˆë¡œìš´ ë°ì´í„°)")
    
    return filtered_urls

#urlë³„ ì„¸ë¶€ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜
async def crawl_brand_detail(url):
    async with async_playwright() as p:
        try:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage']  # ì´ ì˜µì…˜ë“¤ì´ í•„ìš”
            )
            page= await browser.new_page()

            await page.goto(url) #urlë¡œ ì´ë™
            await page.wait_for_load_state("networkidle") #í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        #íšŒì‚¬ ë¦¬ì½œ ë°œí‘œì¼ ì¶”ì¶œ
            company_announcement_element= page.locator("dt:has-text('Company Announcement Date')+dd")
            company_announcement_date_raw = await company_announcement_element.text_content()

            try:
                date_obj=datetime.strptime(company_announcement_date_raw.strip(), "%B %d, %Y")
                company_announcement_date= date_obj.strftime("%Y-%m-%d")
            except ValueError:
                company_announcement_date=company_announcement_date_raw.strip()

            # print(f"íšŒì‚¬ ë¦¬ì½œ ë°œí‘œì¼: {company_announcement_date}")

        #FDA ë¦¬ì½œ ë°œí‘œì¼ ì¶”ì¶œ
            FDA_publish_element= page.locator("dt:has-text('FDA Publish Date') +dd")
            FDA_publish_date_raw= await FDA_publish_element.text_content()

            try: 
                date_obj2= datetime.strptime(FDA_publish_date_raw.strip(), "%B %d, %Y")
                FDA_publish_date= date_obj2.strftime("%Y-%m-%d")
            except ValueError:
                FDA_publish_date= FDA_publish_date_raw.strip()

            # print(f"FDA ë¦¬ì½œ ë°œí‘œì¼: {FDA_publish_date}")

        #íšŒì‚¬ëª… ì¶”ì¶œ
            company_element=page.locator("dt:has-text('Company Name') + dd")
            company_name= await company_element.text_content()

            # print(f"íšŒì‚¬ëª…:{company_name}")

        #ë¸Œëœë“œëª… ì¶”ì¶œ
            try:
                brand_element=page.locator("dt:has-text('Brand Name') + dd .field--item")
                brand_count= await brand_element.text_content()

                if brand_count>0:
                    brand_names=[]
                    for i in range(brand_count):
                        brand_text= await brand_element.nth(i).text_content()
                        if brand_text and brand_text.strip():
                            brand_names.append(brand_text.strip())
                    brand_name= "/".join(brand_names)
                else:
                    brand_name=""
            except:
                brand_name=""

            # print(f"ë¸Œëœë“œëª…:{brand_name}")

        #ë¦¬ì½œì›ì¸ ì¶”ì¶œ
            try:
                recall_reason_element = page.locator("dt:has-text('Product Type') + dd")
                total_recall_reason = await recall_reason_element.text_content()
                
                if total_recall_reason and total_recall_reason.strip():
                    # ë‘ ë²ˆì§¸ ì¤„ì´ë‚˜ ë§ˆì§€ë§‰ ë‹¨ì–´ ì¶”ì¶œ
                    lines = total_recall_reason.strip().split('\n')
                    if len(lines) >= 2:
                        recall_reason = lines[1].strip()  # ë‘ ë²ˆì§¸ ì¤„
                    else:
                        recall_reason = total_recall_reason.split()[-1]  # ë§ˆì§€ë§‰ ë‹¨ì–´
                else:
                    recall_reason = ""
            except Exception as e:
                print(f"âš ï¸ ë¦¬ì½œì›ì¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                recall_reason = ""

            # print(f"ë¦¬ì½œì›ì¸:{recall_reason}")

        #ìì„¸í•œë¦¬ì½œ ì‚¬ìœ  ì¶”ì¶œ
            try:
                recall_reason_detail_element = page.locator("dt:has-text('Reason for Announcement')+ dd .field--item")
                recall_reason_detail = await recall_reason_detail_element.text_content(timeout=5000)
                recall_reason_detail = recall_reason_detail.strip() if recall_reason_detail else ""
            except:
                recall_reason_detail = ""

            # print(f"ìì„¸í•œë¦¬ì½œì‚¬ìœ :{recall_reason_detail}")

        #ì‹í’ˆì¢…ë¥˜ ì¶”ì¶œ
            product_element= page.locator("dt:has-text('Product Description')+dd .field--item")
            product= await product_element.text_content()
            
            # print(f"ì‹í’ˆì¢…ë¥˜:{product}")

        #ë‚´ìš© ì¶”ì¶œ
            content_elements = await page.locator("h2:has-text('Company Announcement') ~ p:not(.inset_column p)").all()
            content_parts = []

            for element in content_elements:
                text = await element.text_content()
                if text and text.strip():
                    content_parts.append(text.strip())

            content = "\n\n".join(content_parts)
            # print(f"ë‚´ìš©: {content}")


            await browser.close()

        #dict í˜•íƒœë¡œ êµ¬ì„±
            recall_data= {
                "document_type": "recall",
                "url": url,
                "company_announcement_date": company_announcement_date,
                "fda_publish_date": FDA_publish_date,
                "company_name": company_name.strip() if company_name else "",
                "brand_name": brand_name.strip() if brand_name else "",
                "recall_reason": recall_reason.strip() if recall_reason else "",
                "recall_reason_detail": recall_reason_detail.strip() if recall_reason_detail else "",
                "product_type": product.strip() if product else "",
                "content": content,
            }

            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {brand_name}")
            print(f"íšŒì‚¬: {company_name}")
            print(f"ë¦¬ì½œ ë°œí‘œì¼: {company_announcement_date}")
            print("-" * 50)

            return recall_data
        except Exception as e:
            print(f" {url} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None


#JSON íŒŒì¼ ì €ì¥ í•¨ìˆ˜ 
def save_to_json(data_list, filename="fda_recalls.json"):
    """í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data_list, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ {len(data_list)}ê°œ ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


#ë©”ì¸í•¨ìˆ˜
async def main():
    print("ğŸš€ ì¦ë¶„ í¬ë¡¤ë§ ì‹œì‘...")
    
    # 1. ì¦ë¶„ ë§í¬ ìˆ˜ì§‘
    brand_urls = await crawl_incremental_links()
    
    # 2. ì¤‘ë³µ ì²´í¬
    filtered_urls = check_existing_urls(brand_urls)
    
    if not filtered_urls:
        print("ğŸ“‹ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“Š ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë°ì´í„°: {len(filtered_urls)}ê°œ")
    
    # 3. ì„¸ë¶€ ì •ë³´ í¬ë¡¤ë§
    all_results = []
    for i, brand_info in enumerate(filtered_urls, 1):
        print(f"\n{i}/{len(filtered_urls)} ì²˜ë¦¬ì¤‘...")
        try:
            result = await crawl_brand_detail(brand_info["url"])
            if result:
                all_results.append(result)
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {brand_info['name']} - {e}")
    
    if not all_results:
        print("âŒ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 4. íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    
    # 5. JSON íŒŒì¼ ì €ì¥ (ê¸°ì¡´ í•¨ìˆ˜ í™œìš©)
    json_filename = f"./data/realtime_recalls_{timestamp}.json"
    save_to_json(all_results, json_filename)
    
    # 6. DB ì €ì¥
    print(f"ğŸ’¾ {len(all_results)}ê°œ ë°ì´í„°ë¥¼ DBì— ì €ì¥ ì¤‘...")
    save_to_sqlite(all_results)
    save_to_chromadb(all_results)
    
    print(f"ğŸ‰ ì¦ë¶„ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   ğŸ“„ JSON: {json_filename}")
    print(f"   ğŸ—„ï¸ SQLite: ./data/fda_recalls.db") 
    print(f"   ğŸ” ChromaDB: ./data/chroma_db_recall")
    print(f"   ğŸ“Š ìƒˆë¡œìš´ ë°ì´í„°: {len(all_results)}ê°œ")

# ì €ì¥ëœ brand_urls.jsonì„ ë¶ˆëŸ¬ì™€ì„œ ìƒì„¸ í¬ë¡¤ë§ë§Œ í•˜ëŠ” í•¨ìˆ˜
async def main_from_saved_urls(json_file):
    """ì €ì¥ëœ brand_urls.jsonì„ ì‚¬ìš©í•´ì„œ ìƒì„¸ í¬ë¡¤ë§ë§Œ ì‹¤í–‰"""
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            brand_urls = json.load(f)
        print(f"ğŸ“‚ ì €ì¥ëœ ë¸Œëœë“œ URL {len(brand_urls)}ê°œë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print("âŒ brand_urls.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € main()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    today=datetime.now().strftime("%m%d")

    all_results = []
    failed_urls = [] 
    for i, brand_info in enumerate(brand_urls, 1):
        print(f"\n {i}/{len(brand_urls)} ì²˜ë¦¬ì¤‘...")
        try:
            result = await crawl_brand_detail(brand_info["url"])
            if result:  # ì„±ê³µí•œ ê²½ìš°ë§Œ
                all_results.append(result)
            else:  # None ë°˜í™˜ ì‹œ ì‹¤íŒ¨ë¡œ ê°„ì£¼
                failed_urls.append(brand_info)
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ : {brand_info['name']} - {e}")
            failed_urls.append(brand_info)

    save_to_json(all_results, f"fda_recalls_{today}.json")
    save_to_json(failed_urls, f"failed_urls2_{today}.json")

    print(f"ğŸ‰ ì´ {len(all_results)}ê°œ ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"âŒ ì‹¤íŒ¨: {len(failed_urls)}ê°œ")

if __name__ == "__main__":
    asyncio.run(main())