# ./fda_realtime_crawler.py
import re
import asyncio
import json
import sqlite3
import os
from datetime import datetime,timedelta
from playwright.async_api import async_playwright
from urllib.parse import urljoin
from db_utils import save_to_sqlite, save_to_chromadb

def get_latest_date_from_db():
    """SQLite DBì—ì„œ ê°€ì¥ ìµœì‹  ë‚ ì§œ ì¡°íšŒ"""
    db_paths_to_try = [
        "./data/fda_recalls.db",
        "/data/fda_recalls.db", 
        "data/fda_recalls.db",
        os.path.join(os.getcwd(), "data", "fda_recalls.db")
    ]
    
    for db_path in db_paths_to_try:
        print(f"ğŸ” ì‹œë„ ì¤‘: {db_path}")
        
        if not os.path.exists(db_path):
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {db_path}")
            continue
            
        file_size = os.path.getsize(db_path)
        print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size} bytes")
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # í…Œì´ë¸” í™•ì¸
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            print(f"ğŸ“Š í…Œì´ë¸”: {tables}")
            
            if ('recalls',) in tables:
                cursor.execute("SELECT fda_publish_date FROM recalls ORDER BY fda_publish_date DESC LIMIT 1")
                result = cursor.fetchone()
                conn.close()
                
                if result and result[0]:
                    print(f"âœ… ìµœì‹  ë‚ ì§œ ì¡°íšŒ ì„±ê³µ: {result[0]}")
                    return result[0]
            
            conn.close()
            
        except Exception as e:
            print(f"ğŸ’¥ DB ì—°ê²° ì˜¤ë¥˜ ({db_path}): {e}")
            continue
    
    print("âŒ ëª¨ë“  ê²½ë¡œì—ì„œ DB ì ‘ê·¼ ì‹¤íŒ¨")
    return None


async def crawl_incremental_links():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
        headless=True,
        args=[
            '--no-sandbox', 
            '--disable-dev-shm-usage',
            '--disable-blink-features=AutomationControlled',  # í•µì‹¬!
            '--disable-web-security',
            '--disable-features=VizDisplayCompositor',
            '--disable-automation',
            '--disable-browser-side-navigation',
            '--disable-dev-shm-usage',
            '--no-first-run'
        ]
    )

        # ì¶”ê°€: ìë™í™” ê°ì§€ ì†ì„± ì œê±°
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
        """)

        # ğŸ†• User-Agent ì¶”ê°€ (ë¸Œë¼ìš°ì € ìœ„ì¥)
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.google.com/',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'cross-site'
        })

        try:
            print("ğŸŒ FDA ì‚¬ì´íŠ¸ ì ‘ì† (í•„í„°ë§ ì—†ì´)...")
            response=await page.goto("https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/")
            print(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status}")
            print(f"ğŸŒ ìµœì¢… URL: {response.url}")   
            await page.wait_for_load_state('networkidle')
            print("âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            print(f"ğŸ’¥ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
            return []

        base_url = "https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/"
        all_brand_urls = []
        current_page_count = 1

        latest_db_date = get_latest_date_from_db()
        print(f"ğŸ“Š DB ìµœì‹  ë‚ ì§œ: {latest_db_date}")
        
        max_pages = 10  # ì•ˆì „ì¥ì¹˜
        while current_page_count <= max_pages:
            print(f"í˜„ì¬ {current_page_count}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")

            print("ğŸ” í˜ì´ì§€ êµ¬ì¡° ë””ë²„ê¹…...")

            # í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€ ì¶œë ¥
            content = await page.content()
            print(f"ğŸ“„ í˜ì´ì§€ í¬ê¸°: {len(content)} ë¬¸ì")
            print(f"ğŸ“ í˜ì´ì§€ ì „ì²´ ë‚´ìš©:")
            print(content)
            print("=" * 50)

            # td ìš”ì†Œ í™•ì¸
            td_count = await page.locator("td").count()
            table_count = await page.locator("table").count()
            print(f"ğŸ“Š table: {table_count}ê°œ, td: {td_count}ê°œ")

            # ì²« ë²ˆì§¸ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ (ì˜µì…˜)
            await page.screenshot(path="debug_page.png")
            print("ğŸ“¸ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ë¨")
            
            # ğŸ†• ì¡°ê±´ë¶€ ë°ì´í„° ìˆ˜ì§‘
            try:
                # ë‚ ì§œ, ë§í¬, Product Type ë™ì‹œ ìˆ˜ì§‘
                date_elements = await page.locator("td:nth-child(1)").all()  # ë‚ ì§œ
                link_elements = await page.locator("td:nth-child(2) a").all()  # ë§í¬  
                product_type_elements = await page.locator("td:nth-child(4)").all()  # Product Type
                
                print(f"ğŸ“Š ë°œê²¬ëœ ìš”ì†Œ: ë‚ ì§œ {len(date_elements)}ê°œ, ë§í¬ {len(link_elements)}ê°œ, ì œí’ˆíƒ€ì… {len(product_type_elements)}ê°œ")
                
            except Exception as e:
                print(f"âš ï¸ ìš”ì†Œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                break
            
            page_has_new_data = False
            should_break = False
            
            # ğŸ¯ ì¡°ê±´ë¶€ ìˆ˜ì§‘ ë¡œì§
            for i in range(min(len(date_elements), len(link_elements), len(product_type_elements))):
                try:
                    # ë‚ ì§œ ì¶”ì¶œ
                    date_text = await date_elements[i].text_content()
                    date_text = date_text.strip()
                    
                    # ë‚ ì§œ ë³€í™˜
                    date_only = None
                    if 'T' in date_text:
                        date_only = date_text.split('T')[0]
                    elif '/' in date_text:
                        try:
                            parsed_date = datetime.strptime(date_text, "%m/%d/%Y")
                            date_only = parsed_date.strftime("%Y-%m-%d")
                        except:
                            date_only = date_text
                    elif '-' in date_text and len(date_text) == 10:
                        date_only = date_text
                    else:
                        continue  # ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    
                    print(f"  ğŸ“… #{i}: ë‚ ì§œ '{date_only}'")
                    
                    # ğŸš¨ DB ìµœì‹  ë‚ ì§œì™€ ë¹„êµ (ë” ì˜¤ë˜ëœ ë°ì´í„°ë©´ ì¤‘ë‹¨)
                    # if latest_db_date and date_only:
                    #     try:
                    #         current_date_obj = datetime.strptime(date_only, "%Y-%m-%d")
                    #         latest_date_obj = datetime.strptime(latest_db_date, "%Y-%m-%d")
                            
                    #         if current_date_obj <= latest_date_obj:
                    #             print(f"ğŸ“Š ê¸°ì¡´ DB ë‚ ì§œ ë„ë‹¬: {date_only} (DB ìµœì‹ : {latest_db_date}) - ì¤‘ë‹¨")
                    #             should_break = True
                    #             break
                    #     except Exception as e:
                    #         print(f"  âš ï¸ ë‚ ì§œ ë¹„êµ ì˜¤ë¥˜: {e}")

                    if latest_db_date and date_only:
                        try:
                            current_date_obj = datetime.strptime(date_only, "%Y-%m-%d")
                            latest_date_obj = datetime.strptime(latest_db_date, "%Y-%m-%d")
                            
                            # âŒ ê¸°ì¡´: DB ë‚ ì§œ ì´ì „ì´ë©´ ì¤‘ë‹¨
                            # if current_date_obj <= latest_date_obj:

                            # âœ… í…ŒìŠ¤íŠ¸ìš©: DB ë‚ ì§œ -3ì¼ ì´ì „ì´ë©´ ì¤‘ë‹¨
                            test_cutoff_date = latest_date_obj - timedelta(days=3)
                            if current_date_obj <= test_cutoff_date:
                                print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ì¡°ê±´ ë„ë‹¬: {date_only} (DB-3ì¼: {test_cutoff_date.strftime('%Y-%m-%d')}) - ì¤‘ë‹¨")
                                should_break = True
                                break
                        except Exception as e:
                            print(f"  âš ï¸ ë‚ ì§œ ë¹„êµ ì˜¤ë¥˜: {e}")
                    
                    # Product Type í™•ì¸
                    product_type_text = await product_type_elements[i].text_content()
                    product_type_text = product_type_text.strip().lower()
                    
                    print(f"  ğŸ·ï¸ #{i}: ì œí’ˆíƒ€ì… '{product_type_text[:50]}...'")
                    
                    # ğŸ¯ Food & Beverages ì •í™•í•œ ì¡°ê±´ í™•ì¸
                    # ì½¤ë§ˆë¡œ ë¶„ë¦¬í•´ì„œ ì²« ë²ˆì§¸ê°€ "Food & Beverages"ì¸ì§€ í™•ì¸
                    product_parts = product_type_text.split(',')
                    first_part = product_parts[0].strip()
                    
                    is_food_beverage = first_part.lower() == "food & beverages"
                    
                    print(f"  ğŸ” #{i}: ì²«ë²ˆì§¸ ë¶€ë¶„ '{first_part}' â†’ {'âœ…' if is_food_beverage else 'âŒ'}")
                    
                    if is_food_beverage:
                        # URL ìˆ˜ì§‘
                        url = await link_elements[i].get_attribute("href")
                        brand_name = await link_elements[i].text_content()
                        full_url = urljoin(base_url, url)
                        
                        all_brand_urls.append({
                            "name": brand_name.strip(), 
                            "url": full_url,
                            "date": date_only,
                            "product_type": product_type_text
                        })
                        page_has_new_data = True
                        print(f"  âœ… ìˆ˜ì§‘: {date_only} - {brand_name.strip()}")
                    else:
                        print(f"  â­ï¸ ìŠ¤í‚µ: Food & Beverages ì•„ë‹˜")
                        
                except Exception as e:
                    print(f"  âš ï¸ í•­ëª© {i} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # ğŸ†• í˜ì´ì§€ë³„ ì¡°ê±´ í™•ì¸ í›„ ë‹¤ìŒ í˜ì´ì§€ ê²°ì •
            if should_break:
                print(f"ğŸ”š ê¸°ì¡´ DB ë‚ ì§œ ë„ë‹¬ë¡œ í¬ë¡¤ë§ ì¢…ë£Œ (í˜ì´ì§€ {current_page_count})")
                break
            
            if not page_has_new_data:
                print(f"ğŸ’¡ í˜ì´ì§€ {current_page_count}ì—ì„œ ìƒˆë¡œìš´ Food & Beverages ë°ì´í„° ì—†ìŒ")
                print(f"ğŸ”š ë” ì´ìƒ ì§„í–‰í•  í•„ìš” ì—†ìŒ - í¬ë¡¤ë§ ì¢…ë£Œ")
                break  # ğŸ†• ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ
            
            # ë‹¤ìŒ í˜ì´ì§€ ì´ë™
            try:
                next_selectors = [
                    "a[rel='next']",
                    ".pager-next a",
                    ".pagination .next a",
                    "a:has-text('Next')",
                    "a:has-text('â€º')"
                ]
                
                next_found = False
                for next_selector in next_selectors:
                    try:
                        next_button = page.locator(next_selector)
                        if await next_button.count() > 0:
                            await next_button.click()
                            await page.wait_for_load_state('networkidle')
                            current_page_count += 1
                            next_found = True
                            print(f"  â¡ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ({current_page_count})")
                            break
                    except:
                        continue
                
                if not next_found:
                    print("ğŸ”š ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ - ì¢…ë£Œ")
                    break
                    
            except Exception as e:
                print(f"ğŸ”š í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ - ì¢…ë£Œ: {e}")
                break
        
        # ê²°ê³¼ ì •ë¦¬
        unique_urls = []
        seen_urls = set()
        
        for item in all_brand_urls:
            if item["url"] not in seen_urls:
                seen_urls.add(item["url"])
                unique_urls.append(item)
        
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"ì´ ì²˜ë¦¬ í˜ì´ì§€: {current_page_count}ê°œ")
        print(f"Food & Beverages ë°ì´í„°: {len(all_brand_urls)}ê°œ")
        print(f"ì¤‘ë³µ ì œê±° í›„: {len(unique_urls)}ê°œ")
        
        await browser.close()
        return unique_urls

def check_existing_urls(new_urls):
    """ê¸°ì¡´ DBì—ì„œ URL ì¤‘ë³µ ì²´í¬"""
    db_path = "./data/fda_recalls.db"
    if not os.path.exists(db_path):
        return new_urls  # DB ì—†ìœ¼ë©´ ëª¨ë“  URL ì²˜ë¦¬
        
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    existing_urls = set()
    for url_info in new_urls:
        cursor.execute("SELECT url FROM recalls WHERE url = ?", (url_info["url"],))
        if cursor.fetchone():
            existing_urls.add(url_info["url"])
    
    conn.close()
    
    # ìƒˆë¡œìš´ URLë§Œ í•„í„°ë§
    filtered_urls = [url_info for url_info in new_urls if url_info["url"] not in existing_urls]
    print(f"ğŸ” ì¤‘ë³µ ì²´í¬: {len(new_urls)}ê°œ â†’ {len(filtered_urls)}ê°œ (ìƒˆë¡œìš´ ë°ì´í„°)")
    
    return filtered_urls

#urlë³„ ì„¸ë¶€ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜
async def crawl_brand_detail(url):
    async with async_playwright() as p:
        try:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage']  # ì´ ì˜µì…˜ë“¤ì´ í•„ìš”
            )
            page= await browser.new_page()

            await page.goto(url) #urlë¡œ ì´ë™
            await page.wait_for_load_state("networkidle") #í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        #íšŒì‚¬ ë¦¬ì½œ ë°œí‘œì¼ ì¶”ì¶œ
            company_announcement_element= page.locator("dt:has-text('Company Announcement Date')+dd")
            company_announcement_date_raw = await company_announcement_element.text_content()

            try:
                date_obj=datetime.strptime(company_announcement_date_raw.strip(), "%B %d, %Y")
                company_announcement_date= date_obj.strftime("%Y-%m-%d")
            except ValueError:
                company_announcement_date=company_announcement_date_raw.strip()

            # print(f"íšŒì‚¬ ë¦¬ì½œ ë°œí‘œì¼: {company_announcement_date}")

        #FDA ë¦¬ì½œ ë°œí‘œì¼ ì¶”ì¶œ
            FDA_publish_element= page.locator("dt:has-text('FDA Publish Date') +dd")
            FDA_publish_date_raw= await FDA_publish_element.text_content()

            try: 
                date_obj2= datetime.strptime(FDA_publish_date_raw.strip(), "%B %d, %Y")
                FDA_publish_date= date_obj2.strftime("%Y-%m-%d")
            except ValueError:
                FDA_publish_date= FDA_publish_date_raw.strip()

            # print(f"FDA ë¦¬ì½œ ë°œí‘œì¼: {FDA_publish_date}")

        #íšŒì‚¬ëª… ì¶”ì¶œ
            company_element=page.locator("dt:has-text('Company Name') + dd")
            company_name= await company_element.text_content()

            # print(f"íšŒì‚¬ëª…:{company_name}")

        #ë¸Œëœë“œëª… ì¶”ì¶œ
            try:
                brand_element=page.locator("dt:has-text('Brand Name') + dd .field--item")
                brand_count= await brand_element.text_content()

                if brand_count>0:
                    brand_names=[]
                    for i in range(brand_count):
                        brand_text= await brand_element.nth(i).text_content()
                        if brand_text and brand_text.strip():
                            brand_names.append(brand_text.strip())
                    brand_name= "/".join(brand_names)
                else:
                    brand_name=""
            except:
                brand_name=""

            # print(f"ë¸Œëœë“œëª…:{brand_name}")

        #ë¦¬ì½œì›ì¸ ì¶”ì¶œ
            try:
                recall_reason_element = page.locator("dt:has-text('Product Type') + dd")
                total_recall_reason = await recall_reason_element.text_content()
                
                if total_recall_reason and total_recall_reason.strip():
                    # ë‘ ë²ˆì§¸ ì¤„ì´ë‚˜ ë§ˆì§€ë§‰ ë‹¨ì–´ ì¶”ì¶œ
                    lines = total_recall_reason.strip().split('\n')
                    if len(lines) >= 2:
                        recall_reason = lines[1].strip()  # ë‘ ë²ˆì§¸ ì¤„
                    else:
                        recall_reason = total_recall_reason.split()[-1]  # ë§ˆì§€ë§‰ ë‹¨ì–´
                else:
                    recall_reason = ""
            except Exception as e:
                print(f"âš ï¸ ë¦¬ì½œì›ì¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                recall_reason = ""

            # print(f"ë¦¬ì½œì›ì¸:{recall_reason}")

        #ìì„¸í•œë¦¬ì½œ ì‚¬ìœ  ì¶”ì¶œ
            try:
                recall_reason_detail_element = page.locator("dt:has-text('Reason for Announcement')+ dd .field--item")
                recall_reason_detail = await recall_reason_detail_element.text_content(timeout=5000)
                recall_reason_detail = recall_reason_detail.strip() if recall_reason_detail else ""
            except:
                recall_reason_detail = ""

            # print(f"ìì„¸í•œë¦¬ì½œì‚¬ìœ :{recall_reason_detail}")

        #ì‹í’ˆì¢…ë¥˜ ì¶”ì¶œ
            product_element= page.locator("dt:has-text('Product Description')+dd .field--item")
            product= await product_element.text_content()
            
            # print(f"ì‹í’ˆì¢…ë¥˜:{product}")

        #ë‚´ìš© ì¶”ì¶œ
            content_elements = await page.locator("h2:has-text('Company Announcement') ~ p:not(.inset_column p)").all()
            content_parts = []

            for element in content_elements:
                text = await element.text_content()
                if text and text.strip():
                    content_parts.append(text.strip())

            content = "\n\n".join(content_parts)
            # print(f"ë‚´ìš©: {content}")


            await browser.close()

        #dict í˜•íƒœë¡œ êµ¬ì„±
            recall_data= {
                "document_type": "recall",
                "url": url,
                "company_announcement_date": company_announcement_date,
                "fda_publish_date": FDA_publish_date,
                "company_name": company_name.strip() if company_name else "",
                "brand_name": brand_name.strip() if brand_name else "",
                "recall_reason": recall_reason.strip() if recall_reason else "",
                "recall_reason_detail": recall_reason_detail.strip() if recall_reason_detail else "",
                "product_type": product.strip() if product else "",
                "content": content,
            }

            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {brand_name}")
            print(f"íšŒì‚¬: {company_name}")
            print(f"ë¦¬ì½œ ë°œí‘œì¼: {company_announcement_date}")
            print("-" * 50)

            return recall_data
        except Exception as e:
            print(f" {url} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None


#JSON íŒŒì¼ ì €ì¥ í•¨ìˆ˜ 
def save_to_json(data_list, filename="fda_recalls.json"):
    """í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data_list, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ {len(data_list)}ê°œ ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


#ë©”ì¸í•¨ìˆ˜
async def main():
    print("ğŸš€ ì¦ë¶„ í¬ë¡¤ë§ ì‹œì‘...")
    
    # 1. ì¦ë¶„ ë§í¬ ìˆ˜ì§‘
    brand_urls = await crawl_incremental_links()
    
    # 2. ì¤‘ë³µ ì²´í¬
    filtered_urls = check_existing_urls(brand_urls)
    
    if not filtered_urls:
        print("ğŸ“‹ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“Š ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë°ì´í„°: {len(filtered_urls)}ê°œ")
    
    # 3. ì„¸ë¶€ ì •ë³´ í¬ë¡¤ë§
    all_results = []
    for i, brand_info in enumerate(filtered_urls, 1):
        print(f"\n{i}/{len(filtered_urls)} ì²˜ë¦¬ì¤‘...")
        try:
            result = await crawl_brand_detail(brand_info["url"])
            if result:
                all_results.append(result)
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {brand_info['name']} - {e}")
    
    if not all_results:
        print("âŒ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 4. íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    
    # 5. JSON íŒŒì¼ ì €ì¥ (ê¸°ì¡´ í•¨ìˆ˜ í™œìš©)
    json_filename = f"./data/realtime_recalls_{timestamp}.json"
    save_to_json(all_results, json_filename)
    
    # 6. DB ì €ì¥
    print(f"ğŸ’¾ {len(all_results)}ê°œ ë°ì´í„°ë¥¼ DBì— ì €ì¥ ì¤‘...")
    save_to_sqlite(all_results)
    save_to_chromadb(all_results)
    
    print(f"ğŸ‰ ì¦ë¶„ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   ğŸ“„ JSON: {json_filename}")
    print(f"   ğŸ—„ï¸ SQLite: ./data/fda_recalls.db") 
    print(f"   ğŸ” ChromaDB: ./data/chroma_db_recall")
    print(f"   ğŸ“Š ìƒˆë¡œìš´ ë°ì´í„°: {len(all_results)}ê°œ")

# ì €ì¥ëœ brand_urls.jsonì„ ë¶ˆëŸ¬ì™€ì„œ ìƒì„¸ í¬ë¡¤ë§ë§Œ í•˜ëŠ” í•¨ìˆ˜
async def main_from_saved_urls(json_file):
    """ì €ì¥ëœ brand_urls.jsonì„ ì‚¬ìš©í•´ì„œ ìƒì„¸ í¬ë¡¤ë§ë§Œ ì‹¤í–‰"""
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            brand_urls = json.load(f)
        print(f"ğŸ“‚ ì €ì¥ëœ ë¸Œëœë“œ URL {len(brand_urls)}ê°œë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print("âŒ brand_urls.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € main()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    today=datetime.now().strftime("%m%d")

    all_results = []
    failed_urls = [] 
    for i, brand_info in enumerate(brand_urls, 1):
        print(f"\n {i}/{len(brand_urls)} ì²˜ë¦¬ì¤‘...")
        try:
            result = await crawl_brand_detail(brand_info["url"])
            if result:  # ì„±ê³µí•œ ê²½ìš°ë§Œ
                all_results.append(result)
            else:  # None ë°˜í™˜ ì‹œ ì‹¤íŒ¨ë¡œ ê°„ì£¼
                failed_urls.append(brand_info)
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ : {brand_info['name']} - {e}")
            failed_urls.append(brand_info)

    save_to_json(all_results, f"fda_recalls_{today}.json")
    save_to_json(failed_urls, f"failed_urls2_{today}.json")

    print(f"ğŸ‰ ì´ {len(all_results)}ê°œ ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"âŒ ì‹¤íŒ¨: {len(failed_urls)}ê°œ")

if __name__ == "__main__":
    asyncio.run(main())