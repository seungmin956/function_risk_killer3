# db_utils.py
import os
import sqlite3
import chromadb
from chromadb.utils import embedding_functions
import uuid
import time
import re
import glob
from datetime import datetime, timedelta
from typing import Dict, Any, List

def save_to_sqlite(data_list: List[Dict], db_path: str = "./data/fda_recalls.db"):
    """
    ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ SQLite DBì— ì§ì ‘ ì €ì¥
    paste-3.txt ë¡œì§ ê¸°ë°˜, JSON íŒŒì¼ ì—†ì´ data_list ì§ì ‘ ì²˜ë¦¬
    """
    
    print(f"ğŸ”„ SQLite ì €ì¥ ì‹œì‘: {len(data_list)}ê°œ ë ˆì½”ë“œ")
    
    # ë°ì´í„° í´ë” ìƒì„±
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
    
    # SQLite ì—°ê²° ë° í…Œì´ë¸” ìƒì„±
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    # í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS recalls (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        document_type TEXT,
        url TEXT UNIQUE,
        company_announcement_date DATE,
        fda_publish_date DATE,
        company_name TEXT,
        brand_name TEXT,
        recall_reason TEXT,
        recall_reason_detail TEXT,
        product_type TEXT,
        content TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """
    cursor.execute(create_table_sql)
    
    # ì¸ë±ìŠ¤ ìƒì„± (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
    indexes = [
        "CREATE INDEX IF NOT EXISTS idx_company_name ON recalls(company_name)",
        "CREATE INDEX IF NOT EXISTS idx_brand_name ON recalls(brand_name)",
        "CREATE INDEX IF NOT EXISTS idx_recall_reason ON recalls(recall_reason)",
        "CREATE INDEX IF NOT EXISTS idx_fda_publish_date ON recalls(fda_publish_date)",
        "CREATE INDEX IF NOT EXISTS idx_product_type ON recalls(product_type)",
        "CREATE INDEX IF NOT EXISTS idx_url ON recalls(url)"
    ]
    
    for index_sql in indexes:
        cursor.execute(index_sql)
    
    # ë°ì´í„° ì‚½ì… SQL
    insert_sql = """
    INSERT OR REPLACE INTO recalls (
        document_type, url, company_announcement_date, fda_publish_date,
        company_name, brand_name, recall_reason, recall_reason_detail,
        product_type, content
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """
    
    converted_count = 0
    for i, record in enumerate(data_list):
        try:
            # í•„ë“œ ë§¤í•‘ ë° ì •ì œ
            cleaned_data = clean_record_for_sqlite(record)
            
            data = (
                cleaned_data['document_type'],
                cleaned_data['url'],
                cleaned_data['company_announcement_date'],
                cleaned_data['fda_publish_date'],
                cleaned_data['company_name'],
                cleaned_data['brand_name'],
                cleaned_data['recall_reason'],
                cleaned_data['recall_reason_detail'],
                cleaned_data['product_type'],
                cleaned_data['content']
            )
            
            cursor.execute(insert_sql, data)
            converted_count += 1
            
        except Exception as e:
            print(f"  âš ï¸ ë ˆì½”ë“œ {i} SQLite ì €ì¥ ì˜¤ë¥˜: {e}")
            print(f"     URL: {record.get('url', 'N/A')}")
            continue
    
    conn.commit()
    conn.close()
    
    print(f"âœ… SQLite ì €ì¥ ì™„ë£Œ: {converted_count}/{len(data_list)}ê°œ ë ˆì½”ë“œ")
    return converted_count

def clean_record_for_sqlite(record: Dict[str, Any]) -> Dict[str, Any]:
    """SQLiteìš© ë ˆì½”ë“œ ì •ì œ"""
    
    cleaned = {}
    
    # í˜„ì¬ JSON êµ¬ì¡° í•„ë“œë“¤ì„ ì§ì ‘ ë§¤í•‘
    cleaned['document_type'] = record.get('document_type', 'recall')
    cleaned['url'] = record.get('url', '')
    cleaned['company_announcement_date'] = record.get('company_announcement_date', None)
    cleaned['fda_publish_date'] = record.get('fda_publish_date', None)
    cleaned['company_name'] = record.get('company_name', '')
    cleaned['brand_name'] = record.get('brand_name', '')
    cleaned['recall_reason'] = record.get('recall_reason', '')
    cleaned['recall_reason_detail'] = record.get('recall_reason_detail', '')
    cleaned['product_type'] = record.get('product_type', '')
    cleaned['content'] = record.get('content', '')
    
    # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ ì •ë¦¬
    for key, value in cleaned.items():
        if value in ['', 'N/A', 'null', None]:
            cleaned[key] = None if key in ['company_announcement_date', 'fda_publish_date'] else ''
    
    # í…ìŠ¤íŠ¸ í•„ë“œ ê¸¸ì´ ì œí•œ (SQLite ì„±ëŠ¥ ê³ ë ¤)
    if cleaned['content'] and len(cleaned['content']) > 15000:
        cleaned['content'] = cleaned['content'][:15000] + '...'
    
    return cleaned

def save_to_chromadb(data_list: List[Dict], 
                    collection_name: str = "FDA_recalls",
                    db_path: str = "./data/chroma_db_recall"):
    """
    ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ChromaDBì— ì§ì ‘ ì €ì¥
    paste-2.txt ë¡œì§ ê¸°ë°˜, JSON íŒŒì¼ ì—†ì´ data_list ì§ì ‘ ì²˜ë¦¬
    """
    
    print(f"ğŸ” ChromaDB ì €ì¥ ì‹œì‘: {len(data_list)}ê°œ ë¬¸ì„œ")
    
    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    chroma_client = chromadb.PersistentClient(path=db_path)
    
    # OpenAI ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
    openai_api_key = os.getenv('OPENAI_API_KEY')
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    basic_ef = embedding_functions.OpenAIEmbeddingFunction(
        api_key=openai_api_key,
        model_name="text-embedding-3-small"
    )
    
    # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
    try:
        collection = chroma_client.get_collection(
            name=collection_name,
            embedding_function=basic_ef
        )
        print(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ì—°ê²°ë¨")
    except:
        collection = chroma_client.create_collection(
            name=collection_name,
            embedding_function=basic_ef,
            metadata={"description": "FDA ë¦¬ì½œ ì‚¬ë¡€ ë°ì´í„° - ì¦ë¶„ ì—…ë°ì´íŠ¸"}
        )
        print(f"ğŸ†• ìƒˆ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„±ë¨")
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
    BATCH_SIZE = 30
    total_chunks = 0
    processed_items = 0
    
    for batch_start in range(0, len(data_list), BATCH_SIZE):
        batch_end = min(batch_start + BATCH_SIZE, len(data_list))
        batch_data = data_list[batch_start:batch_end]
        
        ids = []
        documents = []
        metadatas = []
        
        for i, item in enumerate(batch_data, batch_start):
            try:
                # URLì„ ê³ ìœ  IDë¡œ ì‚¬ìš©
                base_url = item.get("url", f"recall_{i}")
                
                # content í•„ë“œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                content_text = item.get("content", "")
                
                # ìœ íš¨ì„± ì²´í¬
                if not content_text or len(content_text.strip()) < 20:
                    print(f"â— {i}ë²ˆ ë¬¸ì„œ ìŠ¤í‚µë¨ (ë‚´ìš© ì—†ìŒ): {base_url}")
                    continue
                
                # ë¬¸ë‹¨ ê¸°ì¤€ ì²­í‚¹ ì ìš©
                chunks = chunk_content_by_paragraphs(content_text, max_chunk_size=1500, overlap=150)
                
                # ê° ì²­í¬ë§ˆë‹¤ ë³„ë„ ë¬¸ì„œë¡œ ì €ì¥
                for chunk_idx, chunk_content in enumerate(chunks):
                    if len(chunk_content.strip()) < 30:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                        continue
                    
                    # ì²­í¬ë³„ ê³ ìœ  ID ìƒì„±
                    chunk_id = f"{base_url}_chunk_{chunk_idx}" if len(chunks) > 1 else base_url
                    
                    # ê¸°ì¡´ ë¬¸ì„œ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                    try:
                        existing = collection.get(ids=[chunk_id])
                        if existing['ids']:
                            print(f"ğŸ”„ ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸: {chunk_id}")
                            # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ í›„ ìƒˆë¡œ ì¶”ê°€
                            collection.delete(ids=[chunk_id])
                    except:
                        pass  # ìƒˆ ë¬¸ì„œ
                    
                    # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                    raw_metadata = {
                        "document_type": item.get("document_type", "recall"),
                        "url": item.get("url", ""),
                        "company_announcement_date": item.get("company_announcement_date", ""),
                        "fda_publish_date": item.get("fda_publish_date", ""),
                        "company_name": item.get("company_name", ""),
                        "brand_name": item.get("brand_name", ""),
                        "recall_reason": item.get("recall_reason", ""),
                        "recall_reason_detail": item.get("recall_reason_detail", ""),
                        "product_type": item.get("product_type", ""),
                        
                        # ì²­í‚¹ ê´€ë ¨ ë©”íƒ€ë°ì´í„°
                        "chunk_index": chunk_idx,
                        "total_chunks": len(chunks),
                        "is_chunked": len(chunks) > 1
                    }
                    
                    # None ê°’ í•„í„°ë§
                    metadata = filter_none_values(raw_metadata)
                    
                    ids.append(chunk_id)
                    documents.append(chunk_content)
                    metadatas.append(metadata)
                
                processed_items += 1
                
            except Exception as e:
                print(f"í•­ëª© {i} ChromaDB ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì»¬ë ‰ì…˜ì— ì¶”ê°€
        if ids:
            try:
                collection.add(ids=ids, documents=documents, metadatas=metadatas)
                total_chunks += len(ids)
                print(f"ë°°ì¹˜ {batch_start // BATCH_SIZE + 1}: {len(ids)}ê°œ ì²­í¬ ì¶”ê°€")
                time.sleep(1)  # API ë¶€í•˜ ë°©ì§€
            except Exception as e:
                print(f"ë°°ì¹˜ {batch_start // BATCH_SIZE + 1} ChromaDB ì €ì¥ ì˜¤ë¥˜: {e}")
                continue
    
    print(f"âœ… ChromaDB ì €ì¥ ì™„ë£Œ:")
    print(f"   - ì²˜ë¦¬ëœ ë¬¸ì„œ: {processed_items}/{len(data_list)}ê°œ")
    print(f"   - ìƒì„±ëœ ì²­í¬: {total_chunks}ê°œ")
    
    return total_chunks

def chunk_content_by_paragraphs(content_text, max_chunk_size=1500, overlap=200):
    """
    ë¬¸ë‹¨(\n\n) ê¸°ì¤€ìœ¼ë¡œ ì½˜í…ì¸ ë¥¼ ì²­í‚¹í•˜ëŠ” í•¨ìˆ˜ 
    paste-2.txtì—ì„œ ê°€ì ¸ì˜´
    """
    if not content_text or len(content_text.strip()) < 50:
        return [content_text]
    
    # ë¬¸ë‹¨ ë¶„ë¦¬ (\n\n ê¸°ì¤€)
    paragraphs = re.split(r'\n\s*\n', content_text.strip())
    
    # ë¹ˆ ë¬¸ë‹¨ ì œê±°
    paragraphs = [p.strip() for p in paragraphs if p.strip()]
    
    if len(paragraphs) <= 1:
        # ë¬¸ë‹¨ì´ í•˜ë‚˜ë¿ì´ë©´ ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', content_text)
        paragraphs = sentences
    
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        # í˜„ì¬ ì²­í¬ì— ë¬¸ë‹¨ì„ ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° í™•ì¸
        potential_chunk = current_chunk + "\n\n" + paragraph if current_chunk else paragraph
        
        if len(potential_chunk) <= max_chunk_size:
            current_chunk = potential_chunk
        else:
            # í˜„ì¬ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© ê³ ë ¤)
            if overlap > 0 and current_chunk:
                overlap_text = current_chunk[-overlap:].strip()
                current_chunk = overlap_text + "\n\n" + paragraph
            else:
                current_chunk = paragraph
            
            # ë‹¨ì¼ ë¬¸ë‹¨ì´ ë„ˆë¬´ í° ê²½ìš° ê°•ì œ ë¶„í• 
            if len(current_chunk) > max_chunk_size:
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¬ë¶„í• 
                long_sentences = re.split(r'(?<=[.!?])\s+', current_chunk)
                temp_chunk = ""
                
                for sentence in long_sentences:
                    if len(temp_chunk + sentence) <= max_chunk_size:
                        temp_chunk += " " + sentence if temp_chunk else sentence
                    else:
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                        temp_chunk = sentence
                
                current_chunk = temp_chunk
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks if chunks else [content_text]

def filter_none_values(metadata_dict):
    """None ê°’ê³¼ ë¹ˆ ë¬¸ìì—´ì„ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜"""
    filtered = {}
    for key, value in metadata_dict.items():
        if value is not None:
            # ë¹ˆ ë¬¸ìì—´ë„ ì²´í¬
            if isinstance(value, str) and value.strip():
                filtered[key] = value
            elif not isinstance(value, str):
                filtered[key] = value
        # Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° í•´ë‹¹ í‚¤ëŠ” ì œì™¸
    return filtered

def get_recall_stats_from_db(db_path: str = "./data/fda_recalls.db"):
    """SQLite DBì—ì„œ ë¦¬ì½œ í†µê³„ ë°ì´í„° ì¶”ì¶œ"""
    
    if not os.path.exists(db_path):
        return {
            'total_recalls': 0,
            'realtime_recalls': 0,
            'database_recalls': 0,
            'realtime_ratio': 0,
            'latest_crawl': 'ì—†ìŒ'
        }
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # ì´ ë¦¬ì½œ ê±´ìˆ˜
        cursor.execute("SELECT COUNT(*) FROM recalls")
        total_recalls = cursor.fetchone()[0]
        
        # ìµœê·¼ 3ì¼ê°„ ì¶”ê°€ëœ ë°ì´í„° (ì‹¤ì‹œê°„ìœ¼ë¡œ ê°„ì£¼)
        three_days_ago = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')
        cursor.execute("""
            SELECT COUNT(*) FROM recalls 
            WHERE DATE(created_at) >= ?
        """, (three_days_ago,))
        realtime_recalls = cursor.fetchone()[0]
        
        # ê¸°ì¡´ DB ë°ì´í„°
        database_recalls = total_recalls - realtime_recalls
        
        # ì‹¤ì‹œê°„ ë¹„ìœ¨
        realtime_ratio = (realtime_recalls / total_recalls * 100) if total_recalls > 0 else 0
        
        # ìµœê·¼ ì—…ë°ì´íŠ¸ ì‹œê°„
        cursor.execute("""
            SELECT MAX(created_at) FROM recalls
        """)
        latest_result = cursor.fetchone()[0]
        latest_crawl = latest_result if latest_result else 'ì—†ìŒ'
        
        conn.close()
        
        return {
            'total_recalls': total_recalls,
            'realtime_recalls': realtime_recalls,
            'database_recalls': database_recalls,
            'realtime_ratio': realtime_ratio,
            'latest_crawl': latest_crawl
        }
        
    except Exception as e:
        print(f"DB í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {
            'total_recalls': 0,
            'realtime_recalls': 0,
            'database_recalls': 0,
            'realtime_ratio': 0,
            'latest_crawl': 'ì˜¤ë¥˜'
        }

def get_chromadb_stats(db_path: str = "./data/chroma_db_recall", collection_name: str = "FDA_recalls"):
    """ChromaDBì—ì„œ ë¬¸ì„œ ìˆ˜ í™•ì¸"""
    try:
        chroma_client = chromadb.PersistentClient(path=db_path)
        collection = chroma_client.get_collection(collection_name)
        total_documents = collection.count()
        return total_documents
    except Exception as e:
        print(f"ChromaDB ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return 0

def get_visualization_data():
    """ì‹œê°í™”ìš© í†µí•© ë°ì´í„° ë°˜í™˜"""
    try:
        # SQLite í†µê³„
        sqlite_stats = get_recall_stats_from_db()
        
        # ChromaDB í†µê³„
        chromadb_count = get_chromadb_stats()
        
        # ChromaDB ìˆ˜ê°€ ë” ì •í™•í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—…ë°ì´íŠ¸
        if chromadb_count > 0:
            sqlite_stats['total_recalls'] = chromadb_count
        
        # JSON íŒŒì¼ í™•ì¸ (ìµœê·¼ í¬ë¡¤ë§ ì—¬ë¶€)
        data_dir = "./data"
        json_files = []
        if os.path.exists(data_dir):
            import glob
            json_pattern = os.path.join(data_dir, "realtime_recalls_*.json")
            json_files = glob.glob(json_pattern)
            json_files.sort(reverse=True)  # ìµœì‹  íŒŒì¼ ìš°ì„ 
        
        # ìµœê·¼ JSON íŒŒì¼ì´ ìˆìœ¼ë©´ ì‹¤ì‹œê°„ ë°ì´í„°ë¡œ ì¹´ìš´íŠ¸
        if json_files:
            latest_json = json_files[0]
            json_time = os.path.getmtime(latest_json)
            json_datetime = datetime.fromtimestamp(json_time)
            
            # ìµœê·¼ 24ì‹œê°„ ë‚´ íŒŒì¼ì´ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°„ì£¼
            if (datetime.now() - json_datetime).total_seconds() < 86400:  # 24ì‹œê°„
                sqlite_stats['latest_crawl'] = json_datetime.strftime('%Y-%m-%d %H:%M:%S')
        
        return {
            'stats': sqlite_stats,
            'has_data': sqlite_stats['total_recalls'] > 0
        }
        
    except Exception as e:
        print(f"ì‹œê°í™” ë°ì´í„° ìƒì„± ì˜¤ë¥˜: {e}")
        return {
            'stats': {
                'total_recalls': 0,
                'realtime_recalls': 0,
                'database_recalls': 0,
                'realtime_ratio': 0,
                'latest_crawl': 'ì˜¤ë¥˜'
            },
            'has_data': False
        }

def check_new_realtime_data():
    """ìƒˆë¡œìš´ ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸"""
    try:
        data_dir = "./data"
        if not os.path.exists(data_dir):
            return False
            
        # ìµœê·¼ 1ì‹œê°„ ë‚´ ìƒì„±ëœ JSON íŒŒì¼ í™•ì¸
        import glob
        json_pattern = os.path.join(data_dir, "realtime_recalls_*.json")
        json_files = glob.glob(json_pattern)
        
        current_time = datetime.now()
        for json_file in json_files:
            file_time = datetime.fromtimestamp(os.path.getmtime(json_file))
            if (current_time - file_time).total_seconds() < 3600:  # 1ì‹œê°„
                return True
        return False
    except:
        return False